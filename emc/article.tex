\documentclass[12pt,fleqn]{article}
\usepackage{xiiiemc}
\usepackage{natbib}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{wallpaper} 
\usepackage{titlesec}   %% Define space between paragraph e section
\usepackage{float} 	%% Use to fix Figure or Table: ex: \begin{table}[H]
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{hyperref} 	%% Use to fix Figure or Table: ex: \begin{table}[H]
\hypersetup{
	%pagebackref=true,
	pdfcreator={LaTeX with abnTeX2},
	pdfkeywords={abnt}{latex}{abntex}{USPSC}{trabalho acadêmico}, 
	colorlinks=true,       		% false: boxed links; true: colored links
	linkcolor=blue,          	% color of internal links
	citecolor=blue,        		% color of links to bibliography
	filecolor=magenta,      		% color of file links
	urlcolor=blue,
	allbordercolors=black,
	bookmarksdepth=4
}
\usepackage{tocloft}
\titleformat{\section}
  {\normalfont\bfseries}{\thesection.}{0.5em}{}
\renewcommand\cftsecaftersnum{.} 
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
%%%%Don't edit this block. It reduces the spacing between the lines of the references
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{\OLDthebibliography{#1} \setlength{\parskip}{0pt}\setlength{\itemsep}{0pt plus 0.3ex}}

\usepackage{grffile}
\usepackage{afterpage}

%%-----------------------------------------------EDIT-----------------------------------------------
\title{A statistical distance derived from the Kolmogorov-Smirnov test:
	specification, reference measures (benchmarks) and example uses}

%%-----------------------------------------------EDIT----------------------------------------------
\author
    {\rm \begin{tabular}{l} 
    \textbf{Renato Fabbri}$$ - {\textnormal renato.fabbri@gmail.com}\\%
    {\fontsize{11}{0}\selectfont University of São Paulo, Institute of Mathematical and Computer Sciences - São Carlos, SP, Brazil}\vspace*{-0.05cm} \\
%    {\fontsize{11}{0}\selectfont $^{2}$Federal University of ABC, Centre for Natural Sciences and Humanities - São Paulo, SP, Brazil}\vspace*{-0.05cm}\\
  \end{tabular}}
%%----------------------------------------------------------------------------------------------

\fancypagestyle{firspagetstyle}
{
	\lhead{}
	\fancyhead[C]{%
		\includegraphics[width=0.9\linewidth]{logo}\\%
		{\scriptsize \fontfamily{phv}\fontseries{b}\selectfont \color[rgb]{0.45,0.45,0.45}
		16 a 19 de Outubro de 2017\\
		Instituto Politécnico - Universidade do Estado de Rio de Janeiro\\
		Nova Friburgo - RJ\\
	    }
	}
	\renewcommand{\headrulewidth}{0.0pt}
	\fancyfoot[C]{\footnotesize \parbox{15cm} {\centering  \fontsize{7.5}{0}\selectfont \it Anais do XX ENMC – Encontro Nacional de Modelagem Computacional e VIII ECTM – Encontro de Ciências e Tecnologia de Materiais,  Nova Friburgo, RJ – 16 a 19 Outubro 2017}} % \ttfamil
	\rhead{}
}


\begin{document}
\maketitle

\thispagestyle{firspagetstyle}

\fancyhead[L]{\footnotesize{\fontsize{7.5}{0}\selectfont \it XX ENMC e VIII ECTM\\
	16 a 19 de Outubro de 2017\\
	Instituto Politécnico Universidade do Estado do Rio de Janeiro – Nova Friburgo - RJ\\}}
\renewcommand{\headrulewidth}{0.0pt}
\fancyfoot[C]{\footnotesize \parbox{15cm} {\centering  \fontsize{7.5}{0}\selectfont \it Anais do XX ENMC – Encontro Nacional de Modelagem Computacional e VIII ECTM – Encontro de Ciências e Tecnologia de Materiais,  Nova Friburgo, RJ – 16 a 19 Outubro 2017}} % \ttfamil
\rhead{}

\begin{abstract}
Statistical distances quantifies the difference between two statistical constructs.
In this article, we describe reference values for a distance between samples
derived from the Kolmogorov-Smirnov statistic $D_{F,F'}$.
Each measure of the $D_{F,F'}$ is a measure of difference between two samples.
This distance is normalized by the number of observations in each sample
to yield the $c'=D_{F,F'}\sqrt{\frac{n n'}{n+n'}}$ statistic,
for which high levels favor the rejection of the
	null hypothesis (that the samples are drawn from the same distribution).
One great feature of $c'$ is that it inherits the robustness of
	$D_{F,F'}$ and is thus suitable for use in settings where
	the underlying distributions are not known.
Benchmarks are obtained by comparing samples derived from standard distributions.
The supplied example applications of the $c'$ statistic for the distinction
	of samples in real data enables further
insights about the robustness and power of such statistical distance.
\end{abstract}

\keywords{\em{Statistics, Statistical distance, Statistical test, Kolmogorov-Smirnov test, Benchmark}}

\pagestyle{fancy}

\section{INTRODUCTION}\label{sec:intro}
To quantify the difference between samples that are regarded as statistical events,
one can rely in statistical distances.
Such distances are often not metrics, cases in which they do not satify one or more of
the properties of a metric $m$ on samples $x_i \in X$:
\begin{align}
	m(x_i,x_j) &  \geq 0 \\
	m(x_i,x_j) &  = 0 \Leftrightarrow x_i = x_j \\
	m(x_i,x_j) &  = m(x_j,x_i)\\
	m(x_i,x_j) &  \leq m(x_i,x_z) + m(x_z,x_j)
\end{align}

Pseudometrics violate property (1) and/or (2),
quasimetrics violate property (3),
semimetrics violate property (4).
A divergence only satisfies properties (1) and (2).
These are ``generalized metrics''~\citep{wikiStatDist}.

In this article, a statistical distance $c'$ derived from the
Kolmogorov-Smirnov test is described.
The $c'$ statistic can be both a true or a generalized metric,
depending on the implementation details, as explained in Section~\ref{sec:desc}.
To enable the use of the $c'$ metric,
benchmarks are provided
by using standard distributions in various settings and sample sizes.
Example applications of the metric to quantify the difference among
real signals further validate the approach.

Section~\ref{sec:met} describes the metric
and the methods used to characterize it.
Section~\ref{sec:res} is dedicated to
summarizing the results and essential discussions.
Final remarks, including potential future works,
are stated in Section~\ref{sec:conc}.


\section{METHODS}\label{sec:met}
This section describes the $c'$ statistical distance,
the strategy of benchmarking and the validation of $c'$ by means
of application to real samples.


\subsection{Description of the $c'$ statistic}\label{sec:desc}
Be $F$ and $F'$ two empirical cumulative distributions,
where $n$ and $n'$ are the number of observations in each sample.
The two-sample Kolmogorov-Smirnov test rejects the null hypothesis,
that the histograms are the outcome of the same underlying distribution,
if:
\begin{equation}\label{eq:ks}
D_{F,F'} > c(\alpha)\sqrt{\frac{n+n'}{nn'}}
\end{equation}

\noindent where $D_{F,F'}=sup_x[F-F']$ as in Figure~\ref{fig:dnn}
and $c(\alpha)$ is related to the level of significance $\alpha$ by:

\begin{table}[h!]
\centering
\begin{tabular}{|l||c|c|c|c|c|c|}\hline
$\alpha$    & 0.1  & 0.05 & 0.025 & 0.01 & 0.005 & 0.001 \\\hline
$c(\alpha)$ & 1.22 & 1.36 & 1.48  & 1.63 & 1.73  & 1.95  \\\hline
\end{tabular}
\end{table}

If distributions are drawn from empirical data, $D_{F,F'}$ is given as are $n$ and $n'$.
All terms in equation~\ref{eq:ks} are positive and $c(\alpha)$ can be isolated:

\begin{equation}\label{eq:ks2}
%	c(\alpha) < \frac{D_{F,F'}}{\sqrt{\frac{n+n'}{nn'}}} = c
	c(\alpha) < D_{F,F'}\sqrt{\frac{nn'}{n+n'}} = c'
\end{equation}

%Tables~\ref{tab:kolSub}-\ref{tab:kolPctInter} are populated with values for $c'(\alpha)$
The higher $c'$ is, the lower $\alpha$ can be and still entail the rejection of the null hypothesis.

\begin{figure}[!htbp] %h or !htbp
\vspace{-2pt}
\begin{center}
	\includegraphics[width=0.44\textwidth]{../figs/Dnn}
	\caption{The Kolmogorov-Smirnov statistic $D_{F,F'}$: the maximum difference between
		two cumulative distribution functions.}
	\label{fig:dnn}
\end{center}
\end{figure}

In summary,
high values of $c'$ favor rejecting the null hypothesis.
For example, if the significance level is $\alpha=0.01$,
then $c'$ greater than $1.7$
implies the rejection of the null hypothesis,
i.e. implies the assumption that $F$ and $F'$
are outcomes of different distributions.

Of core importance in this study is to regard $c'$
as a measure of distance between both distributions.
If fact, it is a statistical distance.
Following the concepts defined in Section~\ref{sec:intro},
it can be both a true or a generalized metric, depending on the implementation.
It obviously satisfies the Equations (1) and (3).
It satisfies Equation (4) for less obvious reasons.
To grasp how $c'$ satisfies Equation (4),
let $x_i$, $x_j$ and $x_z$ be samples of the same size,
so we only need to compare the $D_{F,F'}$.
Let $F_i$ be the cumulative distribution of the sample $x_i$.
Supose that in the value $\xi$ of X where $F_i$ and $F_j$ are maximally different (i.e. where they yield $D_{F_i,F_j}$),
they are also maximaly different against $F_z$.
If the value of $F_z(\xi)$ is between $F_x(\xi)$ and $F_j(\xi)$:
$D_{F_i,F_z}+D_{F_j,F_z} = D_{F_i,F_j}$,
otherwise:
$D_{F_i,F_z}+D_{F_j,F_z} > D_{F_i,F_j}$.
If the KS statistic $D_{F,F'}$ are not yield at the same value,
it is because they are larger than in the previous cases, thus:
$D_{F_i,F_z}+D_{F_j,F_z} > D_{F_i,F_j}$.
And this completes the argument for:
$D_{F_i,F_z}+D_{F_j,F_z} \geq D_{F_i,F_j}$.
The $c'$ statistic might satisfy or violate Equation (2),
depending on how it is achieved.
If the obtainance of $c'$ depends on making histograms,
than a slightly different observation of a sample might fall under the same bin.
In this case, $c'=m(x_i,x_j) = 0$ and $x_i \neq x_j$, which violates (2)\footnote{If
we instead regard $x_i$ as a histogram, not a sample,
then it satifies (2), but we here assume that $c'$
is in fact a measure related to samples.}.
The cumulative distributions might be derived, however,
not by making a histogram, but simply by ordering the samples~\citep{stack}.
In this case, $c'$ satifies (2).
One exception: if $x_j$ has twice each of the observations in $x_i$,
then it violates (2)
because the distributions entailed by the samples are the same,
but the samples are not the same, and the distance is still zero.

In summary, if $c'$ can be classified both as a metric and a pseudometric,
depending on how it is obtained and theoretical nuances.

\subsection{Benchmarks obtainance}
We considered two cases: when the null hypothesis (that the samples were drawn from the same underlying distributions)
is true and when it is false.
For the case where the null hypothesis is true, 
we compared similar distributions in various settings
many times to assert that we would not assume
that the null hypothesis was false more than
$\alpha . N_c$ where $\alpha$ is the significance level
and $N_c$ is the number of comparisons.
That is, to assert that the Kolmogorov-Smirnov test results
are in accordance with the theory.
In the case where the null hypothesis was false, 
we were interested in measures of $c'$ given that
the null hypothesis is never rejected for a small enough
$\alpha$.
The various measures performed for $c'$ are
described in the results.

One important aspect of the way by which we made the
benchmarks available is that the rendering of the tables
is automated by configurable scripts,
allowing one to obtain tables with other measures
and other comparisons.

\section{RESULTS AND DISCUSSION}\label{sec:res}
This section briefly describes each of the
results, which are: benchmark tables, example uses of $c'$ in
real samples, an exposition of all the data obtained,
and configurable scripts for the generation of all reference tables.

\subsection{When the null hypothesis is true}\label{sec:true}
The theory of the Kolmogorov-Smirnov test
states that one can choose a significance level $\alpha$,
which is the maximum probability that one will reject the
null hypothesis when it is true.
Accordingly,
we rendered tables for each of the distributions:
normal, uniform, 1-parameter Weibull, power function.
Three to five different settings of each of the distributions
were used, both samples had a size of 1000 observations,
and $N_c=100$ comparisons were performed.

Table~\ref{tab:true} is one of such tables.
To understand the columns, notice that
if the null hypothesis is true, the number
of rejections of the null hypothesis ($c'>c(\alpha)$)
in $N_c$ comparisons should not exceed $\alpha . N_c$.
To verify this, let $C=\{c'_i\}$ be a set of $c'$ measures,
and $C(\alpha)=\{c' : c'>c(\alpha)\}$.
Be $|C(\alpha)|$ the cardinality of $C(\alpha)$,
i.e. the number of comparisons in which the two-sample Kolmogorov-Smirnov
test rejects the null hypothesis for a given $\alpha$.

The overall result is that, in fact, the false rejections of the null hypothesis
does not exceed $\alpha . N_c$.
The only exception in our simulations is the power-law (or power function) distribution,
in which the number of rejections of the null hypothesis were usually bellow $\alpha . N_c$
but, in extreme cases, our simulations reached almost $2\alpha N_c$.

\input{../tables/tabWeibullNull_Foo}

\subsection{When the null hypothesis is false}\label{sec:false}
In this case we are interested in measures of $c'$.
The number of comparisons is still $N_c=100$.
The measures on $c'$ chosen to report the results are:
the mean $\mu(c')$, the standard deviation $\sigma(c')$,
the median $m(c')$,
the  fraction
$\overline{C(\alpha)}=\frac{|C(\alpha)|}{N_c}$
of rejection of the null hypothesis given the significance level $\alpha$,
$min(c')$ which states the three smallest values found in the simulations while
$max(c')$ states the three greatest values.
The null hypothesis is true in the boldface lines.
$D$ is the KS statistic when sample size is very large.

Two sets of tables were made to study the $c'$ statistic when
the null hypothesis is false:
\begin{itemize}
	\item Changing the distributions: in each table, the comparisons were made with
one of the distributions remaining unchanged
while the other changes in each row.
Table~\ref{tab:false1} is an example of such table.
	\item Changing the sample sizes:
		changing the number of elements in each sample
		changes the value of the $c'$ statistic. 
		Thus, $c'$ is given for two samples of varied sizes but
		with fixed underlying distributions.         
		Table~\ref{tab:false2} is an example of such table.
\end{itemize}

\input{../tables/tabPowerDiffShape_Foo}
\input{../tables/tabNormalDiffSamples2_Foo}

\subsection{Example application to real samples}
To further validate the $c'$ statistic and enable deeper insights,
a number of applications to real samples were performed:
\begin{itemize}
	\item Texts: Hamlet (Shakespeare), the Bible (KJV), Moby Dick (Herman Melville) and Esaú e Jacó (Machado de Assis),
		where studied by regarding the stopwords and the words which were not stopwords.
		Each of these works were considered as a whole and divided in the first and second half.
                These texts were used to obtain samples that are:
		the mean of the token sizes, the standard variation of the token sizes,
		the token sizes.
		For the two first samples, the text was divided into 1000 parts in which the means
		and standard variations were obtained and regarded as the observations.
		The overall result is: smaller $c'$ for comparisons between parts of the same text
		although high $c'$ was incident even between parts of the same book (especially for the Bible,
		probably because of great differences between the New and Old Testaments).
	\item Audio: the audio segments for testing the sound system of an Ubuntu Linux distribution
		were considered both by their PCM samples and by their Daubechies 8 wavelet coefficients.
		The segments yielded higher $c'$ values as the audio held greater differences, e.g. yield by different words
		or noise.
	\item Music: each classical composition was regarded as a sample
		and the pitches were regarded as observations.
		The results reflect music history.
		For example, measures of $c'$ involving Palestrina
		increases along time with the exception of Beethoven
		who, indeed, used modalism.
		The values of $c'$ related to Bach also increases along time,
		and the outcome of the comparison against Palestrina 
		is only exceeded when Sch\"onberg is reached,
		which reflects the non-tonal discourse of both
		Palestrina and Sch\"onberg.
		Table~\ref{tab:mus} exposes these results.
	\item OS status: workload of the CPUs and memory allocation of the most consuming processes.
		Again, the type of samples are mandatory:
		they might all be identified by the values of $c'$
		found in comparison against other samples,
		with the exception of the RAM memory.
\end{itemize}

\input{../tables/musicDistances_Foo}

\subsection{A thorough exposition of the tables}
These results yield many tables which do not fit this article and would make
this exposition clumsy.
Their thorough exposition are in~\cite{ksstats},
with all the tables and descriptions.

\subsection{Scripts for automated generation of the tables}
The tables that are benchmarks and that result from comparing real samples
are rendered by scripts.
These scripts are configurable, i.e. might be set to render other tables if needed.
Once the new tables are rendered, they might be assembled into a PDF by means
of the latex files that yield~\cite{ksstats}.

\section{CONCLUSIONS AND FURTHER WORK}\label{sec:conc}
This exposition described the $c'$ statistical distance,
the benchmark tables for $c'$ and its use to observe differences in real samples.
As far as I understand, the tables are effective in exposing reference
values in various settings of various distributions.
The Kolmogorv-Smirnov test, from which $c'$ is derived, is known
to be robust in the sense that it is usable even when the underlying distributions
are not known or present problems for other tests.
The overall result is that we obtained a statistical distance which is
useful in various contexts and have now benchmark tables.

Potential next steps are:
\begin{itemize}
	\item better organize the scripts that render the benchmark tables,
		because they are scattered in the \texttt{tests/} directory
		of~\cite{gmaneLegacy}.
	\item Make a better presentation of the benchmark tables in~\cite{ksstats}.
		They are sound but were made for personal usage and might be enhanced
		by better descriptions and contextualization.
	\item Use other distributions for obtaining the tables.
		This is relevant mainly because the number of rejections of the null hypothesis
		was sometimes higher that expected for the significance level in power-law distributions.
	\item Compare $c'$ to other statistical distances: in which cases are they suitable, preferable and
		what results they yield.
	\item Give a more formal account of the conditions needed for $c'$ to be considered a metric 
		and for the cases where $c'$ does not satisfy Equation (2).
	\item Obtain reference values of $c'$ for simulations where the null hypothesis is true.
\end{itemize}
\noindent Finally, the most urgent developments this contribution needs are:
1) a description of the differences in $c'$ in the cases of continuous and discrete distributions;
and 2) implement these measurements without using histograms because they are not needed to
attain the cumulative distribution used for the Kolmogorov-Smirnov statistic and because
$c'$ might be regarded as a metric (not a generalized metric) if obtained without using histograms,
as exposed in Section~\ref{sec:desc}.

\subsection*{\textit{Acknowledgements}}
The author thanks CNPq for the funding received while researching the topic of this article,
the researchers of IFSC/USP and ICMC/USP for the recurrent collaboration in every situation
where we needed directions for investigation.

% ------------------------------------------------------------------------
\begin{thebibliography}{99}
\fontsize{11}{0}\selectfont
\bibitem[Chicheportiche \& Bouchaud, 2012]{kolm}
Chicheportiche, R., \& Bouchaud, J. P. (2012). Weighted Kolmogorov-Smirnov test: Accounting for the tails. Physical Review E, 86(4), 041115.

\bibitem[DrV, 2016]{stack}
	DrV (2016). Python script to obtain the CDF from samples without doing the histogram.
		Stack Overflow.
		From \url{https://stackoverflow.com/questions/24788200/calculate-the-cumulative-distribution-function-cdf-in-python}

\bibitem[Fabbri, 2015]{gmaneLegacy}
	Fabbri, R. (2015). Gmane (legacy): a toolbox for analyzing topology and text of email interaction networks derived from the Gmane database. Github repositories. From \url{https://github.com/ttm/gmaneLegacy}

\bibitem[Fabbri, 2015]{ksstats}
	Fabbri, R. (2015). Benchmark tables for a statistical distance metric derived from the Kolmogorov-Smirnov test. Github repositories. From \url{https://github.com/ttm/kolmogorov-smirnov/raw/master/paper.pdf}

\bibitem[Wikipedia, 2017]{wikiStatDist}
Statistical distance. (2017, August 15). In Wikipedia, The Free Encyclopedia. Retrieved
11:54, August 24, 2017
		, from \url{https://en.wikipedia.org/w/index.php?title=Statistical_distance&oldid=795679286}
\end{thebibliography}
% ------------------------------------------------------------------------

%For papers written in Portuguese or Spanish.

%\begin{center}
%  TITLE IN ENGLISH
%\end{center}

%\def\abstractname{Abstract}%

%\begin{abstract}
%   Abstract in english
%\end{abstract}

%\keywords{\em{Keywords in english}}

\end{document}
